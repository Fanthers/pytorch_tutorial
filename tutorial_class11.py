import torch
import torch.nn as nn


def use_rnncell():
    batch_size = 1
    seq_len = 3
    # å‡å¦‚æˆ‘ä»¬æœ‰id = 1,2,3,4,5,6,7,8,9,10ä¸€å…±10ä¸ªsampleã€‚
    # å‡è®¾æˆ‘ä»¬è®¾å®šseq_lenæ˜¯3ã€‚
    # é‚£ç°åœ¨æ•°æ®çš„å½¢å¼åº”è¯¥ä¸º1-2-3ï¼Œ2-3-4ï¼Œ3-4-5ï¼Œ4-5-6ï¼Œ5-6-7ï¼Œ6-7-8ï¼Œ7-8-9ï¼Œ8-9-10ï¼Œ9-10-0ï¼Œ10-0-0ï¼ˆæœ€åä¸¤ä¸ªæ•°æ®ä¸å®Œæ•´ï¼Œè¿›è¡Œè¡¥é›¶ï¼‰çš„10ä¸ªæ•°æ®ã€‚
    # å‡è®¾æˆ‘ä»¬è®¾å®šbatch_sizeä¸º2ã€‚
    # é‚£æˆ‘ä»¬å–å‡ºç¬¬ä¸€ä¸ªbatchä¸º1-2-3ï¼Œ2-3-4ã€‚è¿™ä¸ªbatchçš„sizeå°±æ˜¯(2ï¼Œ3ï¼Œfeature_dims)äº†ã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªç©æ„å„¿å–‚è¿›æ¨¡å‹ã€‚
    # æ¥ä¸‹æ¥ç¬¬äºŒä¸ªbatchä¸º3-4-5ï¼Œ4-5-6ã€‚
    # ç¬¬ä¸‰ä¸ªbatchä¸º5-6-7ï¼Œ6-7-8ã€‚
    # ç¬¬å››ä¸ªbatchä¸º7-8-9ï¼Œ8-9-10ã€‚
    # ç¬¬äº”ä¸ªbatchä¸º9-10-0ï¼Œ10-0-0ã€‚æˆ‘ä»¬çš„æ•°æ®ä¸€å…±ç”Ÿæˆäº†5ä¸ªbatch
    input_size = 4
    # æ˜¯æŒ‡è¾“å…¥æœ‰å¤šå°‘ç§å­—ç¬¦ï¼Œè€Œä¸æ˜¯å¤šå°‘ä¸ªå­—ç¬¦
    hidden_size = 2

    cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)

    dataset = torch.randn(seq_len, batch_size, input_size)
    hidden = torch.zeros(batch_size, hidden_size)

    for idx, data in enumerate(dataset):
        # idxæŒ‡åºåˆ—
        print('=' * 20, idx, '=' * 20)
        print('Input size:', data.shape, data)

        hidden = cell(data, hidden)

        print('hidden size:', hidden.shape, hidden)
        print(hidden)


def use_rnn():
    batch_size = 1
    seq_len = 3
    input_size = 4
    hidden_size = 2
    num_layers = 1

    cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)

    inputs = torch.randn(seq_len, batch_size, input_size)
    hidden = torch.zeros(num_layers, batch_size, hidden_size)

    out, hidden = cell(inputs, hidden)

    print('Output size:', out.shape)  # (seq_len, batch_size, hidden_size)
    print('Output:', out)
    print('Hidden size:', hidden.shape)  # (num_layers, batch_size, hidden_size)
    print('Hidden:', hidden)


def example_rnncell():
    batch_size = 1
    input_size = 4
    hidden_size = 4

    idx2char = ['e', 'h', 'l', 'o']
    x_data = [1, 0, 2, 2, 3]  # helloä¸­å„ä¸ªå­—ç¬¦çš„ä¸‹æ ‡
    y_data = [3, 1, 2, 3, 2]  # ohlolä¸­å„ä¸ªå­—ç¬¦çš„ä¸‹æ ‡

    one_hot_lookup = [[1, 0, 0, 0],
                      [0, 1, 0, 0],
                      [0, 0, 1, 0],
                      [0, 0, 0, 1]]
    x_one_hot = [one_hot_lookup[x] for x in x_data]  # (seqLen, inputSize)

    inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)
    labels = torch.LongTensor(y_data).view(-1, 1)
    print(inputs.shape, labels.shape)

    class Model(nn.Module):
        def __init__(self, input_size, hidden_size, batch_size):
            super(Model, self).__init__()
            self.batch_size = batch_size
            self.input_size = input_size
            self.hidden_size = hidden_size
            self.rnncell = nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)

        def forward(self, inputs, hidden):
            hidden = self.rnncell(inputs, hidden)
            return hidden

        def init_hidden(self):
            return torch.zeros(self.batch_size, self.hidden_size)

    net = Model(input_size, hidden_size, batch_size)

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=0.1)

    for epoch in range(15):
        loss = 0
        optimizer.zero_grad()
        hidden = net.init_hidden()
        print('Predicted string:', end='')
        for input, label in zip(inputs, labels):
            hidden = net(input, hidden)
            # æ³¨æ„äº¤å‰ç†µåœ¨è®¡ç®—lossçš„æ—¶å€™ç»´åº¦å…³ç³»ï¼Œè¿™é‡Œçš„hiddenæ˜¯([1, 4]), labelæ˜¯ ([1])
            loss += criterion(hidden, label)
            _, idx = hidden.max(dim=1)
            print(idx2char[idx.item()], end='')

        loss.backward()
        optimizer.step()
        print(', Epoch [%d/15] loss=%.4f' % (epoch + 1, loss.item()))


def example_rnn():
    batch_size = 1
    input_size = 4
    hidden_size = 4
    seq_len = 5
    num_layers = 1

    idx2char = ['e', 'h', 'l', 'o']
    x_data = [1, 0, 2, 2, 3]  # helloä¸­å„ä¸ªå­—ç¬¦çš„ä¸‹æ ‡
    y_data = [3, 1, 2, 3, 2]  # ohlolä¸­å„ä¸ªå­—ç¬¦çš„ä¸‹æ ‡

    one_hot_lookup = [[1, 0, 0, 0],
                      [0, 1, 0, 0],
                      [0, 0, 1, 0],
                      [0, 0, 0, 1]]
    x_one_hot = [one_hot_lookup[x] for x in x_data]  # (seqLen, inputSize)

    inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)
    labels = torch.LongTensor(y_data)
    print(inputs.shape, labels.shape)

    class Model(nn.Module):
        def __init__(self, input_size, hidden_size, batch_size, num_layers=1):
            super(Model, self).__init__()
            self.num_layers = num_layers
            self.input_size = input_size
            self.hidden_size = hidden_size
            self.batch_size = batch_size
            self.rnn = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers)

        def forward(self, inputs):
            hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)
            out, _ = self.rnn(inputs, hidden)  # æ³¨æ„è¾“å‡ºç»´åº¦æ˜¯(seqLen, batch_size, hidden_size)
            return out.view(-1, self.hidden_size)  # ä¸ºäº†å®¹æ˜“è®¡ç®—äº¤å‰ç†µè¿™é‡Œè°ƒæ•´ç»´åº¦ä¸º(seqLen * batch_size, hidden_size)

    net = Model(input_size, hidden_size, batch_size)

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=0.1)

    for epoch in range(15):
        optimizer.zero_grad()
        outputs = net(inputs)

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, idx = outputs.max(dim=1)
        idx = idx.data.numpy()
        print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
        print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))


def emb():
    # parameters
    num_class = 4
    input_size = 4
    hidden_size = 8
    embedding_size = 10
    num_layers = 2
    batch_size = 1
    seq_len = 5

    class Model(nn.Module):
        def __init__(self):
            super(Model, self).__init__()
            self.emb = nn.Embedding(input_size, embedding_size)
            self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
            self.fc = nn.Linear(hidden_size, num_class)

        def forward(self, x):
            hidden = torch.zeros(num_layers, x.size(0), hidden_size)
            x = self.emb(x)  # (batch, seqLen, embeddingSize)
            x, _ = self.rnn(x, hidden)  # è¾“å‡º(ğ’ƒğ’‚ğ’•ğ’„ğ’‰ğ‘ºğ’Šğ’›ğ’†, ğ’”ğ’†ğ’’ğ‘³ğ’†ğ’, hidden_size)
            x = self.fc(x)  # è¾“å‡º(ğ’ƒğ’‚ğ’•ğ’„ğ’‰ğ‘ºğ’Šğ’›ğ’†, ğ’”ğ’†ğ’’ğ‘³ğ’†ğ’, ğ’ğ’–ğ’ğ‘ªğ’ğ’‚ğ’”ğ’”)
            return x.view(-1, num_class)  # reshape to use Cross Entropy: (ğ’ƒğ’‚ğ’•ğ’„ğ’‰ğ‘ºğ’Šğ’›ğ’†Ã—ğ’”ğ’†ğ’’ğ‘³ğ’†ğ’, ğ’ğ’–ğ’ğ‘ªğ’ğ’‚ğ’”ğ’”)


    net = Model()

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=0.05)

    idx2char = ['e', 'h', 'l', 'o']
    x_data = [[1, 0, 2, 2, 3]]  # (batch, seq_len)
    y_data = [3, 1, 2, 3, 2]  # (batch * seq_len)

    inputs = torch.LongTensor(x_data)
    labels = torch.LongTensor(y_data)

    for epoch in range(15):
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, idx = outputs.max(dim=1)
        print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
        print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))


if __name__ == '__main__':
    # use_rnncell()
    # use_rnn()
    # example_rnncell()
    # example_rnn()
    emb()